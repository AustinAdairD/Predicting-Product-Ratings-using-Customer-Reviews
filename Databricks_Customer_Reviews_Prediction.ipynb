{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Product Ratings using Customer Reviews (Databricks)"
      ],
      "metadata": {
        "id": "BLdg2Raa7jrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start the timer\n",
        "import time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "ok2Hj3q17ews"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon Reviews Dataset Use Case\n",
        "\n",
        "\n",
        "Source of the data: J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013.\n",
        "\n",
        "#### Loading the DataFrame\n"
      ],
      "metadata": {
        "id": "uTS8K4Rc7tBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import approx_count_distinct, avg, col, date_format, to_date\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import col, split\n",
        "\n",
        "textDF = spark.read.csv(\"/mnt/training/reviews/reviews.csv\", inferSchema = True, header=True, escape='\"')\n",
        "display(textDF.limit(1000))"
      ],
      "metadata": {
        "id": "_aC_Cy7q7qTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take A Random Sample\n"
      ],
      "metadata": {
        "id": "T1wY2MVY8NmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check numbers of rows and columns\n",
        "\n",
        "print((textDF.count(), len(textDF.columns)))"
      ],
      "metadata": {
        "id": "c0YSQr1C8Mzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly sample 1% of the data without replacement\n",
        "\n",
        "sample1 = textDF.sample(False, 0.01, seed=0)\n",
        "\n",
        "sample1 = sample1.select(\"Id\", \"ProductId\", \"Score\", \"Summary\", \"Text\")\n",
        "\n",
        "sample1.cache()\n",
        "\n",
        "print((sample1.count(), len(sample1.columns)))"
      ],
      "metadata": {
        "id": "DNnBPpDW8SNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(sample1)\n"
      ],
      "metadata": {
        "id": "qbLvTLBd8XOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Cleaning Proces\n"
      ],
      "metadata": {
        "id": "FtRV6AIP8ZYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required packages in NLTK\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "azVToXnC8aec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# wrap nltk nlp tokenization and stop word removal in pandas UDF\n",
        "def clean_text(Text):\n",
        "  text = str(Text)\n",
        "  print('text = ', Text)\n",
        "\n",
        "  # split into words\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # convert to lower case\n",
        "  tokens = [w.lower() for w in tokens]\n",
        "\n",
        "  # remove punctuation from each word\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "  # filter out stop words\n",
        "  from nltk.corpus import stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "\n",
        "  # stemming of words\n",
        "  from nltk.stem.porter import PorterStemmer\n",
        "  porter = PorterStemmer()\n",
        "  stemmed = [porter.stem(word) for word in words]\n",
        "  stemmed_minchars = [x for x in stemmed if len(x) > 4]\n",
        "  return(\",\".join(stemmed_minchars))\n",
        "\n",
        "import pandas as pd\n",
        "@pandas_udf('string', PandasUDFType.SCALAR)\n",
        "def clean_text_pandas(v: pd.Series) -> pd.Series: #is a udf\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    return v.map(lambda x: clean_text(x))\n"
      ],
      "metadata": {
        "id": "Z9StMbmm8c9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = sample1.withColumn(\"clean_text\", clean_text_pandas(col(\"Text\"))).withColumn(\"textSWRemoved\", split(col(\"clean_text\"), ',')).drop(\"clean_text\")\n",
        "display(clean_text)"
      ],
      "metadata": {
        "id": "k7r9H4gu8jC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Set params for CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer() \\\n",
        "  .setInputCol(\"textSWRemoved\")     \\\n",
        "  .setOutputCol(\"features\")    \\\n",
        "  .setVocabSize(500)         \\\n",
        "  .setMinDF(3)                 \\\n",
        "  .fit(clean_text)"
      ],
      "metadata": {
        "id": "bacODjz38oZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectors = vectorizer.transform(clean_text).select(\"ProductId\", \"features\", \"Score\")\n",
        "\n",
        "display(count_vectors)"
      ],
      "metadata": {
        "id": "SPrSfTy78o97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "Feature Transformations"
      ],
      "metadata": {
        "id": "5JLDyGrT8ymP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
        "\n",
        "\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"tf_idf\")\n",
        "\n",
        "cleaner = idf.fit(count_vectors)\n",
        "clean_data = cleaner.transform(count_vectors)"
      ],
      "metadata": {
        "id": "pqiLqqT68rkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Traning and Testing data\n"
      ],
      "metadata": {
        "id": "s6EY6yR_89_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(training,testing) = clean_data.randomSplit([0.7,0.3], seed=100)\n"
      ],
      "metadata": {
        "id": "cA6BCHsX871w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creat a Random Forest Classifier\n"
      ],
      "metadata": {
        "id": "U1kilpkr9DVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "nb = RandomForestClassifier(numTrees=3, maxDepth=2, featuresCol=\"tf_idf\", labelCol=\"Score\", seed=100)\n",
        "amazon_predictor = nb.fit(training)"
      ],
      "metadata": {
        "id": "E_XI9FyE9EnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and Load the Model\n"
      ],
      "metadata": {
        "id": "9GFlEB1P9L1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassificationModel\n",
        "\n",
        "fileName = \"/tmp/HW2Q2_TFIDF\"\n",
        "amazon_predictor.write().overwrite().save(fileName)\n",
        "sameModel = RandomForestClassificationModel.load(fileName)"
      ],
      "metadata": {
        "id": "-XN4u_009Mjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Testing Data\n"
      ],
      "metadata": {
        "id": "VLJwfHC09UCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = amazon_predictor.transform(testing)\n",
        "display(test_results)\n"
      ],
      "metadata": {
        "id": "Q9ooMSeQ9O3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n",
        "FMeasure = evaluator.evaluate(test_results)\n",
        "print(\"Weighted F-measure of model at predicting spam was: {}\".format(FMeasure))"
      ],
      "metadata": {
        "id": "3PHN7L7N9ZjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting user rating was: {}\".format(accuracy))"
      ],
      "metadata": {
        "id": "HP_YrgeR9bxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec\n"
      ],
      "metadata": {
        "id": "e82zVuNV9h3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(vectorSize=20, minCount=2, inputCol=\"textSWRemoved\", outputCol=\"w2v\")\n",
        "\n",
        "cleaner = word2vec.fit(clean_text)\n",
        "clean_data = cleaner.transform(clean_text)\n",
        "(training,testing) = clean_data.randomSplit([0.7,0.3], seed=100)"
      ],
      "metadata": {
        "id": "XYx3MlTs9kUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "nb = RandomForestClassifier(numTrees=3, maxDepth=2, featuresCol=\"w2v\", labelCol=\"Score\", seed=100)\n",
        "amazon_predictor = nb.fit(training)\n",
        "\n",
        "test_results = amazon_predictor.transform(testing)"
      ],
      "metadata": {
        "id": "QG6H8rQY9s0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n",
        "FMeasure = evaluator.evaluate(test_results)\n",
        "print(\"Weighted F-measure of model at predicting spam was: {}\".format(FMeasure))"
      ],
      "metadata": {
        "id": "H1903W3d9s-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting user rating was: {}\".format(accuracy))"
      ],
      "metadata": {
        "id": "Rblnb9gM9zka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA Model"
      ],
      "metadata": {
        "id": "6fufjD2090XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "# Trains a LDA model.\n",
        "\n",
        "lda = LDA(k=10, maxIter=10, featuresCol='features')\n",
        "model = lda.fit(count_vectors)\n",
        "\n",
        "transformed = model.transform(count_vectors)\n",
        "\n",
        "clean_data = transformed.select(['Score', 'topicDistribution'])\n",
        "\n",
        "(training,testing) = clean_data.randomSplit([0.7,0.3])"
      ],
      "metadata": {
        "id": "nMg1FBU4BZcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = RandomForestClassifier(numTrees=3, maxDepth=2, featuresCol=\"topicDistribution\", labelCol=\"Score\", seed=100)\n",
        "amazon_predictor = nb.fit(training)\n",
        "\n",
        "test_results = amazon_predictor.transform(testing)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n",
        "FMeasure = evaluator.evaluate(test_results)\n",
        "print(\"Weighted F-measure of model at predicting spam was: {}\".format(FMeasure))\n",
        "Weighted F-measure of model at predicting spam was: 0.513520826020826\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting user rating was: {}\".format(accuracy))"
      ],
      "metadata": {
        "id": "IeX6lcQeBaWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "n_Rt3oX9BjSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA\n",
        "\n",
        "* F Measure: 0.513520826020826\n",
        "\n",
        "* Accuracy Score: 0.6511056511056511\n",
        "\n",
        "\n",
        "\n",
        "### TF-IDF\n",
        "\n",
        "* F measure: 0.49448213840921434\n",
        "\n",
        "* Accuracy Score: 0.6359906213364596\n",
        "\n",
        "\n",
        "\n",
        "### Word2Vec\n",
        "\n",
        "* F measure: 0.5077503769703561\n",
        "\n",
        "* Accuracy Score: 0.6465416178194607"
      ],
      "metadata": {
        "id": "j3phGpaABumL"
      }
    }
  ]
}